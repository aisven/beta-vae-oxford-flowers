---
bibliography: "bibliography.bib"
link-citations: true
urlcolor: "blue"
---

### Total Variation in theory

$$ ğ–³ğ–µ(\mathbb{P}_Î¸,\mathbb{P}_{Î¸\prime}) = \max_{A \subset E}|\mathbb{P}_Î¸(A) - \mathbb{P}_{Î¸\prime}(A)| $$

### Total Variation of continuous probability distributions

$$ ğ–³ğ–µ(\mathbb{P}_Î¸,\mathbb{P}_{Î¸\prime}) = \frac{1}{2} \int_E|f_Î¸(x) - f_{Î¸\prime}(x)|\,dx $$

### Kullback-Leibler divergence aka. relative entropy

$$ ğ–ªğ–«(\mathbb{P}_Î¸,\mathbb{P}_{Î¸\prime}) =  \int_Ef_Î¸(x)\log\left(\frac{f_Î¸(x)}{f_{Î¸\prime}(x)}\right)\,dx $$

- not symmetric (compare to variants of the Jensenâ€“Shannon divergence)
- greater or equal to zero
- definite, i.e. one minimum with $ğ–ªğ–«(\mathbb{P}_Î¸,\mathbb{P}_{Î¸\prime}) = 0$
- not a distance
- a divergence
- can be statistically estimated (by the law of large numbers via an average)
