{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "i4py7m2QjJSa"
      },
      "source": [
        "# Î²-VAE trained on Oxford Flower Image Dataset\n",
        "\n",
        "The purpose of this notebook is simply to generate colorful images based on images of flowers\n",
        "contained in the well-known Oxford Flower Image Dataset."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Disclaimer regarding the code\n",
        "\n",
        "The code has been written in a relatively verbose free style for studying purposes.\n",
        "It is not production-ready and it is not to be regarded as research-related or blog-article code.\n",
        "Inline comments have been written to verbosely describe aspects that came up during coding.\n",
        "Assertions have been added to confirm certain conditions along the way.\n",
        "\n",
        "The author is aware that production-ready code on the other hand would usually, for example,\n",
        "follow principles of clean code, with automated formatting, linting, spell-checking, etc.,\n",
        "contain python type-hints in a much more consistent manner,\n",
        "be automatically tested via test cases written with pytest or similar libraries,\n",
        "be modularized and packaged properly, with selected functions moved to 1st-party libraries,\n",
        "allow for proper packaging, export, import, etc. of readily trained models,\n",
        "be compatible with open source or proprietary machine learning pipelines (think MLOps, DevOps),\n",
        "be integrated with respective tooling to visualize metrics about model performance,\n",
        "ship together with benchmarks, examples, cook book or user guide, etc.,\n",
        "be optimized to some extend."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkGG-QKIjJSb"
      },
      "outputs": [],
      "source": [
        "# in this code block we have all our imports\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# to load PyTorch image dataset\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# to transformation images during loading of PyTorch image dataset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "# to store resulting images\n",
        "#from torchvision.utils import save_image\n",
        "\n",
        "# PyTorch neural network stuff\n",
        "import torch.nn as nn\n",
        "\n",
        "# PyTorch functions\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# to generate filenames for storing trained models or resulting images\n",
        "from datetime import datetime\n",
        "\n",
        "# to plot images or graphs\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# misc\n",
        "import numpy as np\n",
        "#import random\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IftbaEbXjJSc",
        "outputId": "7884f1bb-4914-416d-e18f-b31b1dbbeba7"
      },
      "outputs": [],
      "source": [
        "_# see if GPUs are available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device_str = str(device).upper()\n",
        "print(f\"This computation is running on {device_str}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qi9CVMcOjJSc"
      },
      "outputs": [],
      "source": [
        "# in this code block we define variables mostly regarding dimensionalities\n",
        "\n",
        "# root path of the dataset\n",
        "# to where it is downloaded via the PyTorch dataset facility\n",
        "datasets_root_path = '../../../Datasets/PyTorch'\n",
        "\n",
        "# size of an input image in pixels\n",
        "# as it goes as an input image to the input of the VAE's encoder\n",
        "# note that in this case the images are square\n",
        "# but the VAE code could also work with non-square images\n",
        "n_input_image_pixels_height = 136\n",
        "# for now we use smaller images for faster training during development\n",
        "#n_input_image_pixels_height = 96\n",
        "n_input_image_pixels_width = n_input_image_pixels_height\n",
        "\n",
        "# number of channels in an image\n",
        "# in this case one channel each for red, green, blue\n",
        "n_image_channels = 3\n",
        "\n",
        "# number of feature images obtained from one image after the convolutional layers in the encoder\n",
        "n_dim_feature_images = 32\n",
        "\n",
        "# number of pixels the convolutions cut away from the image height and width\n",
        "n_pixels_conv_cutaway = 8\n",
        "\n",
        "# size of an output image in pixels\n",
        "# as it comes as a generated image from the output of the VAE's decoder\n",
        "n_output_image_pixels_height = n_input_image_pixels_height - n_pixels_conv_cutaway\n",
        "n_output_image_pixels_width = n_input_image_pixels_width - n_pixels_conv_cutaway\n",
        "\n",
        "# number of features before mapping to the latent space\n",
        "n_dim_features = n_dim_feature_images * n_output_image_pixels_height * n_output_image_pixels_width\n",
        "\n",
        "# the number of dimensions of the latent space\n",
        "# aka. number of latent variables\n",
        "# which is then also the number of values in a vector z in the latent space\n",
        "n_dim_latent_space=64\n",
        "\n",
        "# should a DataLoader shuffle the input images\n",
        "# or rather always yield them in the same order\n",
        "shuffle = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mHtNbqAjJSd"
      },
      "outputs": [],
      "source": [
        "# in this code block we define a custom transformation regarding gamma, brightness, saturation\n",
        "\n",
        "class CustomImageTransform:\n",
        "    \"\"\"Adust an image in a custom way.\"\"\"\n",
        "\n",
        "    def __call__(self, img):\n",
        "        img = TF.adjust_gamma(img, 1.6, 1.1)\n",
        "        img = TF.adjust_brightness(img, 0.7)\n",
        "        img = TF.adjust_saturation(img, 1.5)\n",
        "        return img\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTbrsl2tjJSe"
      },
      "outputs": [],
      "source": [
        "# in this code block we define our image transformation pipeline treating the images a bit\n",
        "\n",
        "# note that some magic numbers in here\n",
        "# are just temporary sizes of input images as they go through the pre-processing pipeline\n",
        "# and that we could extract these magic numbers into variables or even calculate them\n",
        "# but for the sake of simplicity we just use magic numbers here\n",
        "\n",
        "# pre-processing pipeline to pimp these images from 2009 and earlier a bit\n",
        "transformations_enhance = transforms.Compose([\n",
        "                                      transforms.Resize(384, transforms.InterpolationMode.BICUBIC, antialias=True),\n",
        "                                      transforms.CenterCrop([320, 320]),\n",
        "                                      transforms.GaussianBlur(kernel_size=3.0,sigma=2.0),\n",
        "                                      CustomImageTransform(),\n",
        "                                      transforms.RandomAutocontrast(p=1.0),\n",
        "                                      transforms.RandomAdjustSharpness(sharpness_factor=4, p=1.0),\n",
        "                                      transforms.GaussianBlur(kernel_size=1.5,sigma=1.5),\n",
        "                                      transforms.Resize(256, transforms.InterpolationMode.BICUBIC, antialias=True),\n",
        "                                      transforms.CenterCrop([n_input_image_pixels_height, n_input_image_pixels_width]),\n",
        "                                      transforms.ToTensor()\n",
        "                                    ])\n",
        "\n",
        "# for faster training switch to this much simpler pre-processing pipeline\n",
        "transformations_justcrop = transforms.Compose([\n",
        "                                      transforms.Resize(256, transforms.InterpolationMode.BICUBIC, antialias=True),\n",
        "                                      transforms.CenterCrop([n_input_image_pixels_height, n_input_image_pixels_width]),\n",
        "                                      transforms.ToTensor()\n",
        "                                    ])\n",
        "\n",
        "transformations = transformations_enhance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_URMTHrjJSe",
        "outputId": "7ccad9ab-af75-4923-cfc6-b7e19cddecee"
      },
      "outputs": [],
      "source": [
        "# in this code block we define the train, val, test dataset\n",
        "\n",
        "dataset_train = datasets.Flowers102(\n",
        "    root=datasets_root_path,\n",
        "    split='train',\n",
        "    download=True,\n",
        "    transform=transformations\n",
        ")\n",
        "\n",
        "dataset_val = datasets.Flowers102(\n",
        "    root=datasets_root_path,\n",
        "    split='val',\n",
        "    download=True,\n",
        "    transform=transformations\n",
        ")\n",
        "\n",
        "dataset_test = datasets.Flowers102(\n",
        "    root=datasets_root_path,\n",
        "    split='test',\n",
        "    download=True,\n",
        "    transform=transformations\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAaSR9e9jJSf"
      },
      "outputs": [],
      "source": [
        "# in this code block we just run such a custom data loader once to view an example input image\n",
        "\n",
        "view_example_image = False\n",
        "\n",
        "if view_example_image:\n",
        "    dataloader_example1 = DataLoader(dataset_train, batch_size=1, shuffle=shuffle)\n",
        "    images_example1, labels_example1 = next(iter(dataloader_example1))\n",
        "    print(f\"images_example1.size()={images_example1.size()}\")\n",
        "    print(f\"labels_example1.size()={labels_example1.size()}\")\n",
        "    image_example1 = images_example1[0]\n",
        "    label_example1 = labels_example1[0]\n",
        "    print(f\"image_example1.size()={image_example1.size()}\")\n",
        "    print(f\"image_example1={image_example1}\")\n",
        "    print(f\"label_example1.size()={label_example1.size()}\")\n",
        "    print(f\"label_example1={label_example1}\")\n",
        "    # use .permute(1, 2, 0) to move the channel values from the first to the last dimension\n",
        "    image_example1 = image_example1\n",
        "    plt.imshow(image_example1.permute(1, 2, 0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATKriQ_EjJSg"
      },
      "outputs": [],
      "source": [
        "# in this code block we define a helper function to generate filenames for storing trained models\n",
        "\n",
        "def create_filename(prefix: str, extension: str):\n",
        "    now = datetime.now()\n",
        "    now_str = now.strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"{prefix}_{now_str}.{extension}\"\n",
        "    return filename\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RYXRc2rjJSh"
      },
      "outputs": [],
      "source": [
        "# in this code block we define our Convolutional Beta Variational Auto-Encoder\n",
        "\n",
        "# convolutional Î²-VAE\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self,\n",
        "                 n_image_channels,\n",
        "                 n_dim_feature_images,\n",
        "                 conv_kernel_size,\n",
        "                 n_dim_features,\n",
        "                 n_dim_latent_space,\n",
        "                 n_output_image_pixels_height,\n",
        "                 n_output_image_pixels_width,\n",
        "                 beta: torch.Tensor,\n",
        "                 debug: bool,\n",
        "                 trace: bool):\n",
        "        super(VAE, self).__init__()\n",
        "        \n",
        "        self.n_image_channels = n_image_channels\n",
        "        self.n_dim_feature_images = n_dim_feature_images\n",
        "        self.n_dim_inner_conv = int(self.n_dim_feature_images / 2)\n",
        "        print(f\"self.n_dim_inner_conv={self.n_dim_inner_conv}\")\n",
        "        self.conv_kernel_size = conv_kernel_size\n",
        "        self.n_dim_features = n_dim_features\n",
        "        self.n_dim_latent_space = n_dim_latent_space\n",
        "        self.n_output_image_pixels_height = n_output_image_pixels_height\n",
        "        self.n_output_image_pixels_width = n_output_image_pixels_width\n",
        "        self.beta = beta\n",
        "        self.debug = debug\n",
        "        self.trace = trace\n",
        "\n",
        "        print(f\"Creating new Beta-VAE instance with beta={beta.cpu()}.\")\n",
        "\n",
        "        # encoder layers\n",
        "\n",
        "        # input to the encoder passes through\n",
        "        # two convolutional layers to work out important features in each input image\n",
        "        self.encoder_layer_1_conv = nn.Conv2d(in_channels=self.n_image_channels, out_channels=self.n_dim_inner_conv, kernel_size=self.conv_kernel_size)\n",
        "        torch.nn.init.xavier_uniform_(self.encoder_layer_1_conv.weight)\n",
        "        self.encoder_layer_2_conv = nn.Conv2d(in_channels=self.n_dim_inner_conv, out_channels=self.n_dim_feature_images, kernel_size=self.conv_kernel_size)\n",
        "        torch.nn.init.xavier_uniform_(self.encoder_layer_2_conv.weight)\n",
        "        # # note that in the forward pass of the encoder\n",
        "        # the output of the last convolutional layer\n",
        "        # is first reshaped into a vector before it is then passed to the first linear layer\n",
        "\n",
        "        # and two linear layers to compute the parameters of distributions in the latent space where\n",
        "        # computed vectors mu and sigma_log parameterize these distributions of latent variables\n",
        "        # in fact there is one linear layer to compute the vector of means mu\n",
        "        self.encoder_layer_3_linear = nn.Linear(in_features=self.n_dim_features, out_features=self.n_dim_latent_space)\n",
        "        torch.nn.init.xavier_uniform_(self.encoder_layer_3_linear.weight)\n",
        "        # and one linear layer to compute the vector of log std sigma_log\n",
        "        # note that the output of this layer is interpreted/used as\n",
        "        # the natural logarithm of each computed standard deviation\n",
        "        # the reason why it is interpreted as log std instead of just std is that\n",
        "        # in this way the network can more easily cover a broad range of values\n",
        "        # so the natural logarithm (log aka. ln) appears here sort of as a numerical compression\n",
        "        self.encoder_layer_4_linear = nn.Linear(in_features=self.n_dim_features, out_features=self.n_dim_latent_space)\n",
        "        torch.nn.init.xavier_uniform_(self.encoder_layer_4_linear.weight)\n",
        "\n",
        "        # decoder layers\n",
        "\n",
        "        # z is first passed through a linear layer to go from the latent space to the feature space\n",
        "        # note that in the forward pass of the decoder\n",
        "        # the output of this layer needs to be reshaped\n",
        "        self.decoder_layer_1_linear = nn.Linear(in_features=self.n_dim_latent_space, out_features=self.n_dim_features)\n",
        "        torch.nn.init.xavier_uniform_(self.decoder_layer_1_linear.weight)\n",
        "        # before it then is passed to the first \"de-convolutional\" layer\n",
        "        self.decoder_layer_2_deconv = nn.ConvTranspose2d(in_channels=self.n_dim_feature_images, out_channels=self.n_dim_inner_conv, kernel_size=self.conv_kernel_size)\n",
        "        torch.nn.init.xavier_uniform_(self.decoder_layer_2_deconv.weight)\n",
        "        self.decoder_layer_3_deconv = nn.ConvTranspose2d(in_channels=self.n_dim_inner_conv, out_channels=self.n_image_channels, kernel_size=self.conv_kernel_size)\n",
        "        torch.nn.init.xavier_uniform_(self.decoder_layer_3_deconv.weight)\n",
        "\n",
        "\n",
        "    def encoder(self, x):\n",
        "        assert device_str == 'CPU' or x.is_cuda\n",
        "        # the convolutional layers are traversed one after the other\n",
        "        # thereby the activation function is mainly used to add non-linearity as usual\n",
        "        # in this case GELU (Gaussian Error Linear Units) is used\n",
        "        # in fact the tanh-based approximation for increased computational speed\n",
        "\n",
        "        if self.debug:\n",
        "            print(f\"x.shape={x.shape} before convolutional layers.\")\n",
        "\n",
        "        x = F.gelu(input=self.encoder_layer_1_conv(x), approximate='tanh')\n",
        "        assert device_str == 'CPU' or x.is_cuda\n",
        "        if self.debug:\n",
        "            print(f\"x.shape={x.shape} after 1st convolutional layer.\")\n",
        "\n",
        "        x = F.gelu(input=self.encoder_layer_2_conv(x), approximate='tanh')\n",
        "        assert device_str == 'CPU' or x.is_cuda\n",
        "        if self.debug:\n",
        "            print(f\"x.shape={x.shape} after 2nd convolutional layer.\")\n",
        "\n",
        "        # now make a vector out of what we currently have computed\n",
        "        x = x.view(-1, self.n_dim_features)\n",
        "        assert device_str == 'CPU' or x.is_cuda\n",
        "        if self.debug:\n",
        "            print(f\"x.shape={x.shape} after reshaping.\")\n",
        "\n",
        "        # compute vector of means\n",
        "        mu = self.encoder_layer_3_linear(x)\n",
        "        assert device_str == 'CPU' or mu.is_cuda\n",
        "        assert self.n_dim_latent_space == mu.shape[1]\n",
        "        if self.debug:\n",
        "            print(f\"mu.shape={mu.shape}\")\n",
        "\n",
        "        # compute vector of log variances\n",
        "        sigma_log = self.encoder_layer_4_linear(x)\n",
        "        assert device_str == 'CPU' or sigma_log.is_cuda\n",
        "        assert self.n_dim_latent_space == sigma_log.shape[1]\n",
        "        if self.debug:\n",
        "            print(f\"sigma_log.shape={sigma_log.shape}\")\n",
        "    \n",
        "        return mu, sigma_log\n",
        "\n",
        "\n",
        "    def re_parametrization_trick(self, mu, sigma_log):\n",
        "        # do the VAE re-parametrization trick\n",
        "        # z = mu + (sigma * epsilon)\n",
        "        # where epsilon will be sampled from a standard normal distribution\n",
        "\n",
        "        # first scale the log standard deviations into standard deviations\n",
        "        # by applying the exponential function with base e\n",
        "        sigma = torch.exp( sigma_log / 2)\n",
        "\n",
        "        # note that the vector sigma is only passed to the PyTorch function randn_like\n",
        "        # to tell it which dimensionality the result should have\n",
        "        epsilon = torch.randn_like(sigma)\n",
        "        epsilon.to(device)\n",
        "\n",
        "        assert self.n_dim_latent_space == mu.shape[1]\n",
        "        assert self.n_dim_latent_space == sigma_log.shape[1]\n",
        "        assert sigma.shape == sigma_log.shape\n",
        "        assert epsilon.shape == sigma.shape\n",
        "\n",
        "        assert device_str == 'CPU' or mu.is_cuda\n",
        "        assert device_str == 'CPU' or sigma_log.is_cuda\n",
        "        assert device_str == 'CPU' or sigma.is_cuda\n",
        "        assert device_str == 'CPU' or epsilon.is_cuda\n",
        "\n",
        "        # so here is the linear combination of the vector of learned means\n",
        "        # and the corresponding vector of learned standard deviations\n",
        "        # with the scaling factor epsilon being a random fraction\n",
        "        # drawn from the standard normal distribution N(0,1)\n",
        "        # to obtain the vector z\n",
        "        random_deviation = sigma * epsilon\n",
        "        assert device_str == 'CPU' or random_deviation.is_cuda\n",
        "        z = mu + random_deviation\n",
        "        assert device_str == 'CPU' or z.is_cuda\n",
        "\n",
        "        if self.trace:\n",
        "            print(f\"sigma=\\n{sigma.cpu()}\")\n",
        "            print(f\"epsilon=\\n{epsilon.cpu()}\")\n",
        "            print(f\"random_deviation=\\n{random_deviation.cpu()}\")\n",
        "            print(f\"mu=\\n{mu.cpu()}\")\n",
        "            print(f\"z=\\n{z.cpu()}\")\n",
        "\n",
        "        if self.debug:\n",
        "            print(f\"z.shape={z.cpu().shape}\")\n",
        "\n",
        "        # to obtain values so to say sampled from the learned distributions in the latent space\n",
        "        return z\n",
        "\n",
        "\n",
        "    def decoder(self, z):\n",
        "        # decoding\n",
        "        x = F.gelu(input=self.decoder_layer_1_linear(z), approximate='tanh')\n",
        "        assert device_str == 'CPU' or x.is_cuda\n",
        "        x = x.view(-1, self.n_dim_feature_images, self.n_output_image_pixels_height, self.n_output_image_pixels_width)\n",
        "        assert device_str == 'CPU' or x.is_cuda\n",
        "        x = F.gelu(input=self.decoder_layer_2_deconv(x), approximate='tanh')\n",
        "        assert device_str == 'CPU' or x.is_cuda\n",
        "        x = torch.sigmoid(self.decoder_layer_3_deconv(x))\n",
        "        assert device_str == 'CPU' or x.is_cuda\n",
        "        return x\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # encoding -> re-parametrization -> decoding\n",
        "        # this is a full forward pass mostly only used in training of the VAE\n",
        "        mu, log_sigma = self.encoder(x)\n",
        "        assert device_str == 'CPU' or mu.is_cuda\n",
        "        assert device_str == 'CPU' or log_sigma.is_cuda\n",
        "        z = self.re_parametrization_trick(mu, log_sigma)\n",
        "        assert device_str == 'CPU' or z.is_cuda\n",
        "        output = self.decoder(z)\n",
        "        assert device_str == 'CPU' or output.is_cuda\n",
        "        # returns decoder output, means, log variances for subsequent computation of loss\n",
        "        return output, mu, log_sigma\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRjApwQrjJSi",
        "outputId": "1f9f8ef4-cc75-44ca-ae6f-7a48612e2a6d"
      },
      "outputs": [],
      "source": [
        "# in this code block we initialize a Î²-VAE for subsequent training\n",
        "\n",
        "print(f\"n_image_channels={n_image_channels}\")\n",
        "print(f\"n_dim_feature_images={n_dim_feature_images}\")\n",
        "print(f\"n_dim_features={n_dim_features}\")\n",
        "print(f\"n_dim_latent_space={n_dim_latent_space}\")\n",
        "\n",
        "#torch.cuda.empty_cache()\n",
        "\n",
        "beta = torch.Tensor([4.0])\n",
        "beta = beta.to(device)\n",
        "assert beta.shape == (1,)\n",
        "assert device_str == 'CPU' or beta.is_cuda\n",
        "\n",
        "vae = VAE(n_image_channels=n_image_channels,\n",
        "          n_dim_feature_images=n_dim_feature_images,\n",
        "          conv_kernel_size=5,\n",
        "          n_dim_features=n_dim_features,\n",
        "          n_dim_latent_space=n_dim_latent_space,\n",
        "          n_output_image_pixels_height=n_output_image_pixels_height,\n",
        "          n_output_image_pixels_width=n_output_image_pixels_width,\n",
        "          beta=beta,\n",
        "          debug=False,\n",
        "          trace=False)\n",
        "\n",
        "vae.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4g6fB37jJSi"
      },
      "outputs": [],
      "source": [
        "# in this code block we define a procedure to train a Î²-VAE\n",
        "\n",
        "def train_vae_variant_1(vae: VAE,\n",
        "                        lr: float,\n",
        "                        n_epochs: int,\n",
        "                        n_epochs_max: int,\n",
        "                        dataloader_train: DataLoader,\n",
        "                        n_batches_max: int,\n",
        "                        debug: bool):\n",
        "    # initialize the optimizer\n",
        "    optimizer = torch.optim.Adam(vae.parameters(), lr=lr)\n",
        "\n",
        "    # initialize the KL-divergence function\n",
        "    kl_loss = nn.KLDivLoss(reduction=\"batchmean\", log_target=True)\n",
        "\n",
        "    # iterate\n",
        "    for epoch in range(n_epochs):\n",
        "        if debug:\n",
        "            print(f\"train epoch={epoch} starting.\")    \n",
        "\n",
        "        # iterate over the batches of images\n",
        "        dataloader_train_iterator = iter(dataloader_train)\n",
        "        batch_ctr = 0\n",
        "        batch_ctr_max_reached = False\n",
        "        output_images = None\n",
        "        # note that the underscore is just to ignore any labels the DataLoader provides\n",
        "        # since auto-encoders are generally unsupervised and do not need labels\n",
        "        # however the dataset does contain labels so we just ignore them here\n",
        "        # the underscore is just for discarding the labels\n",
        "        for batch_of_input_images, _ in dataloader_train_iterator:\n",
        "            #if debug and epoch % 10 == 0:\n",
        "            #    print(f\"train epoch={epoch} batch={batch_ctr} starting.\")    \n",
        "            \n",
        "            # move the batch to GPU if available\n",
        "            batch_of_input_images = batch_of_input_images.to(device)\n",
        "                \n",
        "            # forward pass to generate image and obtain the vectors mu and sigma_log\n",
        "            output_images, mu, sigma_log = vae(batch_of_input_images)\n",
        "\n",
        "            #print(f\"output_images.is_cuda={output_images.is_cuda}\")\n",
        "            assert device_str == 'CPU' or output_images.is_cuda\n",
        "            #print(f\"mu.is_cuda={mu.is_cuda}\")\n",
        "            assert device_str == 'CPU' or mu.is_cuda\n",
        "            assert device_str == 'CPU' or vae.beta.is_cuda\n",
        "            #print(f\"sigma_log.is_cuda={sigma_log.is_cuda}\")\n",
        "            assert device_str == 'CPU' or sigma_log.is_cuda\n",
        "\n",
        "            # VAE loss is binary cross-entropy loss as reconstruction term\n",
        "            bce_loss = F.binary_cross_entropy(output_images, batch_of_input_images, reduction='sum')\n",
        "            #print(f\"bce_loss.is_cuda={bce_loss.is_cuda}\")\n",
        "            assert device_str == 'CPU' or bce_loss.is_cuda\n",
        "            # combined with KullbackâLeibler divergence as regularization\n",
        "            # note that this form of the KL-divergence is simplified\n",
        "            # based on the facts that the distributions are normal distributions\n",
        "            # and the prior distributions are standard normal distributions\n",
        "            kl_divergence = 0.5 * torch.sum(-1 - sigma_log + mu.pow(2) + sigma_log.exp())\n",
        "            #print(f\"kl_divergence.is_cuda={kl_divergence.is_cuda}\")\n",
        "            assert device_str == 'CPU' or kl_divergence.is_cuda\n",
        "            # now Î²-VAE loss is just that with the constant beta > 1 emphasizing regularization\n",
        "            # in order to force better disentanglement of the features\n",
        "            # by improving the properties of the learned distributions in the latent space\n",
        "            loss = bce_loss + (vae.beta * kl_divergence)\n",
        "            #print(f\"loss.is_cuda={loss.is_cuda}\")\n",
        "            assert device_str == 'CPU' or loss.is_cuda\n",
        "\n",
        "            #print(f\"train epoch={epoch} batch={batch_ctr} loss={loss.item()}\"\n",
        "            #      f\" = bce_loss={bce_loss.item()} + (beta={vae.beta.item()} * kl_divergence={kl_divergence.item()})\")    \n",
        "    \n",
        "            # back-propagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            batch_ctr = batch_ctr + 1\n",
        "            if(n_batches_max <= batch_ctr):\n",
        "                batch_ctr_max_reached = True\n",
        "                break\n",
        "\n",
        "        print(f\"train epoch={epoch} loss = {loss.cpu()} = {bce_loss.cpu()} + {vae.beta.cpu()} * {kl_divergence.cpu()})\")\n",
        "        if(batch_ctr_max_reached):\n",
        "            print(f\"train reached maximum number of batches {n_batches_max}.\")\n",
        "            break\n",
        "        if(n_epochs_max <= epoch):\n",
        "            print(f\"train reached maximum number of epochs {n_epochs_max}.\")\n",
        "            break\n",
        "\n",
        "    # ensure output Tensor is detached from autograd before returning\n",
        "    output_images = output_images.detach()\n",
        "    return output_images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NUdD1kBjJSj",
        "outputId": "9fb1881e-8475-4d2a-e963-c6db60d6ed81"
      },
      "outputs": [],
      "source": [
        "lr = 0.0003\n",
        "n_epochs = 30\n",
        "n_epochs_max = 30\n",
        "batch_size_train = 32\n",
        "n_batches_max = 100000000\n",
        "debug = False\n",
        "\n",
        "assert 2 <= batch_size_train\n",
        "assert batch_size_train <= 128\n",
        "\n",
        "dataloader_train = DataLoader(dataset=dataset_train, batch_size=batch_size_train, shuffle=True, pin_memory=True)\n",
        "\n",
        "output_images = train_vae_variant_1(vae=vae,\n",
        "                                    lr=lr,\n",
        "                                    n_epochs=n_epochs,\n",
        "                                    n_epochs_max=n_epochs_max,\n",
        "                                    dataloader_train=dataloader_train,\n",
        "                                    n_batches_max=n_batches_max,\n",
        "                                    debug=debug)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 912
        },
        "id": "I45pl35mjJSj",
        "outputId": "aaf1ce48-bea9-4074-c283-aaf73aea7ea0"
      },
      "outputs": [],
      "source": [
        "output_images_iterator = iter(output_images)\n",
        "output_image_0 = output_images[0].cpu()\n",
        "print(f\"output_image_0.is_cuda={output_image_0.is_cuda}\")\n",
        "print(f\"output_image_0.shape={output_image_0.shape}\")\n",
        "print(f\"output_image_0={output_image_0}\")\n",
        "plt.imshow(output_image_0.permute(1, 2, 0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "02D_PLGbjJSj",
        "outputId": "b2d08143-f0ff-4296-9bb7-17ea48688ceb"
      },
      "outputs": [],
      "source": [
        "plt.imshow(next(output_images_iterator).cpu().permute(1, 2, 0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FERf8Kee_nKA"
      },
      "outputs": [],
      "source": [
        "# in this code block we prepare a forward pass with test images\n",
        "\n",
        "dataloader_test = DataLoader(dataset_test, batch_size=128, shuffle=True, pin_memory=True)\n",
        "dataloader_test_iterator = iter(dataloader_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFbfLni7DJrV"
      },
      "outputs": [],
      "source": [
        "# in this code block we do a forward pass with test images\n",
        "\n",
        "batch_of_input_images_test, _ = next(dataloader_test_iterator)\n",
        "batch_of_input_images_test = batch_of_input_images_test.to(device)\n",
        "batch_of_input_images_test_clone = batch_of_input_images_test.clone()\n",
        "batch_of_input_images_test_clone_iterator = iter(batch_of_input_images_test_clone)\n",
        "output_images_test, mu_test, log_sigma_test = vae.forward(batch_of_input_images_test)\n",
        "output_images_test_iterator = iter(output_images_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "usHt-uHhOFZs",
        "outputId": "021fedef-9553-4bdb-ec90-9e9d6c976a6c"
      },
      "outputs": [],
      "source": [
        "# in this code block we show the next test input image\n",
        "\n",
        "# given all above code block ran\n",
        "# the user can run this code block to see the next input image\n",
        "# and subsequently run the next code block to see the corresponding result image\n",
        "# so the user may alternately run this code block and the next code block\n",
        "# to get this experience\n",
        "\n",
        "input_image_test_clone = next(batch_of_input_images_test_clone_iterator).detach().cpu()\n",
        "plt.imshow(input_image_test_clone.permute(1, 2, 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "Zgi0m_h8EeuD",
        "outputId": "1cf5a685-3443-4616-f648-e9c4c9773354"
      },
      "outputs": [],
      "source": [
        "# in this code block we show each test output image per run of this cell\n",
        "\n",
        "output_image_test = next(output_images_test_iterator).detach().cpu()\n",
        "plt.imshow(output_image_test.permute(1, 2, 0))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "b76bbc844b550c21ecc5000efe9bb58828bcfe5d07d905b729a3ddd117bb6f93"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
